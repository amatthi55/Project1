{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bureau of Labor Statistics API to create csv files for the US weekly wages of various age groups over a 15 year time span. The wages are unadjusted for inflation and they are from the BLS's Current Population Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-34.csv\n",
      "    general     men   women years\n",
      "0    576.00  617.50  512.00  2001\n",
      "1    590.50  628.00  530.00  2002\n",
      "2    594.25  629.00  546.25  2003\n",
      "3    603.75  638.75  561.25  2004\n",
      "4    610.00  644.00  572.00  2005\n",
      "5    622.00  662.00  583.75  2006\n",
      "6    643.00  686.75  597.50  2007\n",
      "7    665.25  704.00  624.50  2008\n",
      "8    678.25  715.00  634.25  2009\n",
      "9    682.25  713.50  648.75  2010\n",
      "10   691.75  716.75  661.75  2011\n",
      "11   706.50  737.50  666.75  2012\n",
      "12   708.25  743.00  665.25  2013\n",
      "13   725.25  755.00  679.25  2014\n",
      "14   734.75  771.25  689.50  2015\n",
      "35-44.csv\n",
      "    general     men   women years\n",
      "0    657.00  754.00  546.50  2001\n",
      "1    668.50  759.50  569.75  2002\n",
      "2    687.25  775.25  590.25  2003\n",
      "3    712.75  803.50  608.00  2004\n",
      "4    731.00  823.25  621.00  2005\n",
      "5    747.50  836.00  645.25  2006\n",
      "6    769.50  872.75  667.25  2007\n",
      "7    804.00  914.50  681.75  2008\n",
      "8    817.75  916.00  709.00  2009\n",
      "9    823.25  914.50  730.75  2010\n",
      "10   837.25  935.50  733.75  2011\n",
      "11   858.00  956.50  746.75  2012\n",
      "12   874.00  956.75  766.75  2013\n",
      "13   881.00  963.00  781.00  2014\n",
      "14   899.50  983.75  803.25  2015\n",
      "45-54.csv\n",
      "    general      men   women years\n",
      "0    693.25   799.50  587.50  2001\n",
      "1    705.75   807.50  600.75  2002\n",
      "2    722.75   834.25  608.50  2003\n",
      "3    742.75   856.50  625.75  2004\n",
      "4    748.00   853.50  643.25  2005\n",
      "5    775.00   896.50  659.50  2006\n",
      "6    790.25   908.00  677.50  2007\n",
      "7    822.50   944.25  706.00  2008\n",
      "8    838.25   966.50  712.25  2009\n",
      "9    844.00   954.00  729.50  2010\n",
      "10   866.25   979.50  743.00  2011\n",
      "11   878.00   993.50  745.50  2012\n",
      "12   883.75   994.75  761.75  2013\n",
      "13   898.50  1011.50  779.00  2014\n",
      "14   923.25  1038.75  799.75  2015\n",
      "55-64.csv\n",
      "    general      men   women years\n",
      "0    637.75   763.50  540.25  2001\n",
      "1    674.00   802.00  572.25  2002\n",
      "2    708.50   828.00  601.00  2003\n",
      "3    724.00   842.50  615.75  2004\n",
      "4    742.75   853.50  638.50  2005\n",
      "5    765.25   903.75  658.25  2006\n",
      "6    801.75   932.00  680.00  2007\n",
      "7    825.00   942.25  711.00  2008\n",
      "8    840.50   964.50  726.25  2009\n",
      "9    860.75   979.75  736.00  2010\n",
      "10   880.00   997.50  748.75  2011\n",
      "11   897.50  1005.50  766.00  2012\n",
      "12   903.50  1013.75  779.50  2013\n",
      "13   910.50  1021.50  780.75  2014\n",
      "14   927.00  1067.75  783.75  2015\n",
      "65&Over.csv\n",
      "    general      men   women years\n",
      "0    488.50   568.25  391.25  2001\n",
      "1    502.50   573.25  431.00  2002\n",
      "2    517.75   610.75  431.50  2003\n",
      "3    554.00   636.75  476.25  2004\n",
      "4    566.75   651.00  491.75  2005\n",
      "5    583.25   652.50  512.00  2006\n",
      "6    606.00   683.00  536.75  2007\n",
      "7    641.25   754.25  560.00  2008\n",
      "8    683.75   787.75  604.75  2009\n",
      "9    683.00   794.00  599.75  2010\n",
      "10   742.50   839.00  664.75  2011\n",
      "11   752.75   859.50  664.50  2012\n",
      "12   804.00   937.00  690.25  2013\n",
      "13   827.00   943.25  728.50  2014\n",
      "14   870.00  1002.75  736.00  2015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#BLS is the Bureau of Labor Statistics' wrapper for their API.\n",
    "import bls\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import API_Key\n",
    "\n",
    "bls.api.set_api_key(API_Key)\n",
    "\n",
    "#years is used as a column in the data frames later in the script\n",
    "years = ['2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "\n",
    "\n",
    "#age_buckets is a list of lists containing the the Burea of Labor Statistics' ID numbers for various wage datasets\\\n",
    "# related to age and gender. In each element of age_buckets is a list of data set IDs and each of these IDs refer\\\n",
    "# to the overall, male, and female wage datasets for each age group.\n",
    "age_buckets = [['LEU0252888500','LEU0252888700','LEU0252888900','25-34.csv'],['LEU0252889100','LEU0252889300','LEU0252889500','35-44.csv'], \\\n",
    "               ['LEU0252889700','LEU0252889900','LEU0252890100','45-54.csv'],['LEU0252890900', 'LEU0252891100', 'LEU0252891300','55-64.csv'],\\\n",
    "               ['LEU0252891500','LEU0252891700','LEU0252891900','65&Over.csv']]\n",
    "\n",
    "#loop through age_buckets and create a separate csv for each age bucket\n",
    "for age_bucket in age_buckets:\n",
    "    \n",
    "    #request a certain data set from BLS and specificy the years of data\n",
    "\n",
    "    #first will be assigned the median US wages for the entire age bucket over the years\n",
    "    first = (bls.get_series(\n",
    "            age_bucket[0], startyear=2001, endyear=2015\n",
    "        ))\n",
    "\n",
    "    #second will be assigned the median US wages for males in the age bucket over the years\n",
    "    second = (bls.get_series(\n",
    "            age_bucket[1], startyear=2001, endyear=2015\n",
    "        ))\n",
    "\n",
    "    #third will be assigned the median US wages for females in the age bucket over the years\n",
    "    third = (bls.get_series(\n",
    "            age_bucket[2], startyear=2001, endyear=2015\n",
    "        ))\n",
    "\n",
    "\n",
    "    #first, second, and third are combined into a dataframe\n",
    "    DF = pd.DataFrame({'general':first, \"men\": second, 'women':third})\n",
    "\n",
    "\n",
    "    #The datasets returned by the BLS website return wages for Q1, Q2, Q3, and Q4 of the year \\\n",
    "    #Here we average the median wages for each quarter in each year and create a new DF that has wages by year\n",
    "\n",
    "    #This collects the annual median wages in this age bucket\n",
    "    general = []\n",
    "    #This collects the annual median wages for men in this age bucket\n",
    "    men = []\n",
    "    #This collects the annual median wages for women in this age bucket\n",
    "    women = []\n",
    "\n",
    "    gen = 0\n",
    "    m = 0\n",
    "    w = 0\n",
    "    c = 1\n",
    "\n",
    "\n",
    "    #loop through each row in DF\n",
    "    for i in range(len(DF['general'])):\n",
    "\n",
    "        #add the quarterly wages from the different rows to find the annual wage\n",
    "        gen +=DF['general'][i]\n",
    "        m +=DF['men'][i]\n",
    "        w +=DF['women'][i]\n",
    "\n",
    "        #every fourth row execute the below code\n",
    "        if c % 4 == 0:   \n",
    "            #Take gen, which is the sum of the median wages for each quarter in the year, \\\n",
    "            # and divide by 4 to find the average of the median wages and then append that \\\n",
    "            # to general. The same thing is repeated for men and women.\n",
    "            general.append(gen/4)\n",
    "            men.append(m/4)\n",
    "            women.append(w/4)\n",
    "\n",
    "            #reset variables for next year of wages\n",
    "            gen = 0\n",
    "            m = 0\n",
    "            w = 0\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    #create new DF that shows annual median wages for the age bucket\n",
    "    DF = pd.DataFrame({'years':years, 'general':general, \"men\": men, 'women':women})\n",
    "\n",
    "    DF.to_csv('Age_Bucket_Data/{}'.format(age_bucket[3]))\n",
    "    print(age_bucket[3])\n",
    "    print(DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is used to merge several dataframes taken from the BLS. The BLS dataframes show weeking earnings in the US by race and by gender. Each dataframe is for one year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Characteristic  2002_median_weekly_earnings 2002_workers_in_000's\n",
      "0  Total, 16 years and over                          608               100,081\n",
      "1    Men, 16 years and over                          679                56,345\n",
      "2       Men, 16 to 24 years                          391                 6,317\n",
      "3    Men, 25 years and over                          732                50,027\n",
      "4  Women, 16 years and over                          529                43,737\n",
      "             Characteristic  2002_median_weekly_earnings  \\\n",
      "0  Total, 16 years and over                          608   \n",
      "1    Men, 16 years and over                          679   \n",
      "2       Men, 16 to 24 years                          391   \n",
      "3    Men, 25 years and over                          732   \n",
      "4  Women, 16 years and over                          529   \n",
      "\n",
      "  2002_workers_in_000's  2003_median_weekly_earnings 2003_workers_in_000's  \\\n",
      "0               100,081                          620               100,302   \n",
      "1                56,345                          695                56,227   \n",
      "2                 6,317                          398                 6,158   \n",
      "3                50,027                          744                50,069   \n",
      "4                43,737                          552                44,076   \n",
      "\n",
      "   2004_median_weekly_earnings 2004_workers_in_000's  \\\n",
      "0                          638               101,224   \n",
      "1                          713                57,001   \n",
      "2                          400                 6,243   \n",
      "3                          762                50,758   \n",
      "4                          573                44,223   \n",
      "\n",
      "   2005_median_weekly_earnings 2005_workers_in_000's  \\\n",
      "0                          651               103,560   \n",
      "1                          722                58,406   \n",
      "2                          409                 6,396   \n",
      "3                          771                52,010   \n",
      "4                          585                45,154   \n",
      "\n",
      "   2006_median_weekly_earnings             ...              \\\n",
      "0                          671             ...               \n",
      "1                          743             ...               \n",
      "2                          418             ...               \n",
      "3                          797             ...               \n",
      "4                          600             ...               \n",
      "\n",
      "  2013_workers_in_000's  2013_median_weekly_earnings 2014_workers_in_000's  \\\n",
      "0               104,262                         $776               106,526   \n",
      "1                57,994                          860                59,450   \n",
      "2                 5,207                          479                 5,493   \n",
      "3                52,787                          912                53,957   \n",
      "4                46,268                          706                47,076   \n",
      "\n",
      "   2014_median_weekly_earnings 2015_workers_in_000's  \\\n",
      "0                         $791               109,080   \n",
      "1                          871                60,746   \n",
      "2                          493                 5,476   \n",
      "3                          922                55,270   \n",
      "4                          719                48,334   \n",
      "\n",
      "   2015_median_weekly_earnings 2016_workers_in_000's  \\\n",
      "0                         $809               111,091   \n",
      "1                          895                61,930   \n",
      "2                          510                 5,646   \n",
      "3                          947                56,284   \n",
      "4                          726                49,161   \n",
      "\n",
      "  2016_median_weekly_earnings 2017_workers_in_000's  \\\n",
      "0                         832               113,272   \n",
      "1                         915                62,980   \n",
      "2                         512                 5,791   \n",
      "3                         969                57,190   \n",
      "4                         749                50,291   \n",
      "\n",
      "  2017_median_weekly_earnings  \n",
      "0                         860  \n",
      "1                         941  \n",
      "2                         547  \n",
      "3                         996  \n",
      "4                         770  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "#This list is used to loop through all of the race/wage CSVs\n",
    "years = ['2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017']\n",
    "\n",
    "#Read in each CSV and merge it on the \"Characteric' column\n",
    "Race_DF = pd.read_csv('../Raw_Data/Race_Age_Data/2002_Race_Age.csv')\n",
    "print(Race_DF.head())\n",
    "\n",
    "for year in years:\n",
    "    Race_DF = pd.merge(Race_DF,pd.read_csv('../Raw_Data/Race_Age_Data/{}_Race_Age.csv'.format(year)), on = 'Characteristic')\n",
    "\n",
    "print(Race_DF.head())\n",
    "\n",
    "#Final dataframe\n",
    "Race_DF.to_csv('Race_DataFrame.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code cleans data about wages in certain US industries and data about the number of employees\n",
    "in those industries. All of the csv files that are read in for this data cleaning process are downloaded from the \n",
    "BLS's website. The Current Population Survey is the name of the survey that collected the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing file path locations\n",
    "\n",
    "#The Age/Profession csv files contain breakdowns of the size of age groups(e.g. 25-34, 35-44)\\\n",
    "#for a variety of industries in the US.\n",
    "age_2017 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2017.xlsx\"\n",
    "age_2016 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2016.xlsx\"\n",
    "age_2015 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2015.xlsx\"\n",
    "age_2014 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2014.xlsx\"\n",
    "age_2013 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2013.xlsx\"\n",
    "age_2012 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2012.xlsx\"\n",
    "age_2011 = \"../Raw_Data/Gender_Age_Data/Age/Profession_Age_2011.xlsx\"\n",
    "#The Gender/Profession csv files contain data on the median wages for a variety of industries in\\\n",
    "#the US. Data for female wages in an industry and male wages in an industry is also given.\n",
    "gender_2017 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2017.xlsx\"\n",
    "gender_2016 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2016.xlsx\"\n",
    "gender_2015 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2015.xlsx\"\n",
    "gender_2014 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2014.xlsx\"\n",
    "gender_2013 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2013.xlsx\"\n",
    "gender_2012 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2012.xlsx\"\n",
    "gender_2011 = \"../Raw_Data/Gender_Age_Data/Gender/Profession_Gender_2011.xlsx\"\n",
    "\n",
    "\n",
    "#Creating Age DataFrames\n",
    "Age_2017_df = pd.read_excel(age_2017)\n",
    "Age_2016_df = pd.read_excel(age_2016)\n",
    "Age_2015_df = pd.read_excel(age_2015)\n",
    "Age_2014_df = pd.read_excel(age_2014)\n",
    "Age_2013_df = pd.read_excel(age_2013)\n",
    "Age_2012_df = pd.read_excel(age_2012)\n",
    "Age_2011_df = pd.read_excel(age_2011)\n",
    "#Creating Gender DataFrames\n",
    "Gender_2017_df = pd.read_excel(gender_2017)\n",
    "Gender_2016_df = pd.read_excel(gender_2016)\n",
    "Gender_2015_df = pd.read_excel(gender_2015)\n",
    "Gender_2014_df = pd.read_excel(gender_2014)\n",
    "Gender_2013_df = pd.read_excel(gender_2013)\n",
    "Gender_2012_df = pd.read_excel(gender_2012)\n",
    "Gender_2011_df = pd.read_excel(gender_2011)\n",
    "\n",
    "#test show header\n",
    "#Age_2016_df.head()\n",
    "Gender_2016_df.head()\n",
    "\n",
    "#Merging Age and Gender DataFrames on Occupation\n",
    "Profession_2017_df = pd.merge(Age_2017_df, Gender_2017_df, on=\"Occupation\")\n",
    "Profession_2016_df = pd.merge(Age_2016_df, Gender_2016_df, on=\"Occupation\")\n",
    "Profession_2015_df = pd.merge(Age_2015_df, Gender_2015_df, on=\"Occupation\")\n",
    "Profession_2014_df = pd.merge(Age_2014_df, Gender_2014_df, on=\"Occupation\")\n",
    "Profession_2013_df = pd.merge(Age_2013_df, Gender_2013_df, on=\"Occupation\")\n",
    "Profession_2012_df = pd.merge(Age_2012_df, Gender_2012_df, on=\"Occupation\")\n",
    "Profession_2011_df = pd.merge(Age_2011_df, Gender_2011_df, on=\"Occupation\")\n",
    "\n",
    "\n",
    "#Testing DataFrame Merge\n",
    "Profession_2017_df.head()\n",
    "\n",
    "\n",
    "#DataMunging\n",
    "\n",
    "#Since there are hundreds of occupations listed in the above dataframe,\\\n",
    "#we decided to focus on a few industries that we are interested in. Industries\\\n",
    "#that we wanted to explore were given a value in the Industry_x column and all of\\\n",
    "#the other cells are left blank.\n",
    "#This code removes all the rows that have a blank in the Industry_x column, \\\n",
    "#which leaves only the rows that we have marked to include in the final analysis.\n",
    "Prof_2017 = Profession_2017_df[Profession_2017_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2016 = Profession_2016_df[Profession_2016_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2015 = Profession_2015_df[Profession_2015_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2014 = Profession_2014_df[Profession_2014_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2013 = Profession_2013_df[Profession_2013_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2012 = Profession_2012_df[Profession_2012_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "Prof_2011 = Profession_2011_df[Profession_2011_df.Industry_x.str.contains(\"NaN\")==False]\n",
    "\n",
    "#Final Data Munging\n",
    "Clean_Prof_2017 = Prof_2017.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2016 = Prof_2016.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2015 = Prof_2015.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2014 = Prof_2014.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2013 = Prof_2013.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2012 = Prof_2012.replace({'–':'0','-':'0'}, regex=True)\n",
    "Clean_Prof_2011 = Prof_2011.replace({'–':'0','-':'0'}, regex=True)\n",
    "\n",
    "\n",
    "#Exporting Dataframe to CSV\n",
    "Clean_Prof_2017.to_csv(\"Profession_Age_Gender_2017.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2016.to_csv(\"Profession_Age_Gender_2016.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2015.to_csv(\"Profession_Age_Gender_2015.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2014.to_csv(\"Profession_Age_Gender_2014.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2013.to_csv(\"Profession_Age_Gender_2013.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2012.to_csv(\"Profession_Age_Gender_2012.csv\", encoding = 'utf-8', index = False)\n",
    "Clean_Prof_2011.to_csv(\"Profession_Age_Gender_2011.csv\", encoding = 'utf-8', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for reading in and cleaning NBA wage data.\n",
    "The clean data is returned in the file \"NBA_clean.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Data Cleaning ran successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#====================================================================================================\n",
    "#=======================Building Dataset for yearly salaries with Player name as Key=================\n",
    "#====================================================================================================\n",
    "\n",
    "historic_bball = '../nbaResources/nba_salaries_1990_to_2018.csv'\n",
    "\n",
    "# Set empty list variables\n",
    "Player= []\n",
    "Salary = []\n",
    "Season = []\n",
    "\n",
    "#Open current NBA CSV to zip important values into a df\n",
    "with open(historic_bball, 'r') as csvFile:                    \n",
    "\n",
    "   csvReader = csv.reader(csvFile, delimiter=',')\n",
    "\n",
    "   #Skip headers\n",
    "   next(csvReader, None)                                      \n",
    "\n",
    "   for row in csvReader:\n",
    "\n",
    "       #Append data from the row\n",
    "       Player.append(row[0])\n",
    "       Salary.append(row[1])\n",
    "       Season.append(row[2])\n",
    "    \n",
    "#Zip lists together\n",
    "cleanCSV = list(zip(Player, Salary, Season))\n",
    "#cleanCSV                        #Returns raw data for processing into dataframe             \n",
    "\n",
    "\n",
    "df = pd.DataFrame(cleanCSV)              #creating dataframe for\\\n",
    "df.columns=['Player','Salary','Season']  #changing column name\n",
    "df.head()\n",
    "#df.count\n",
    "\n",
    "#====================================================================================================\n",
    "#==================Building Dataset for Current Players Ages with Player name as Key=================\n",
    "#====================================================================================================\n",
    "\n",
    "bball_age = '../nbaResources/Age_player.csv'\n",
    "\n",
    "#Set empty list variables\n",
    "Player= []\n",
    "Age2018 = []\n",
    "\n",
    "#Open current NBA CSV to zip important Values into a df\n",
    "with open(bball_age, 'r') as csvFile:\n",
    "   csvReader = csv.reader(csvFile, delimiter=',')\n",
    "\n",
    "   #Skip headers\n",
    "   next(csvReader, None)\n",
    "\n",
    "   for row in csvReader:\n",
    "\n",
    "       #Append data from the row\n",
    "       Player.append(row[0])\n",
    "       Age2018.append(row[1])\n",
    "\n",
    "       \n",
    "#Zip lists together\n",
    "AgeCSV = list(zip(Player, Age2018))\n",
    "#AgeCSV\n",
    "\n",
    "playerAge_df = pd.DataFrame(AgeCSV)\n",
    "playerAge_df.columns=['Player','Age']  #changing column name\n",
    "#playerAge_df.describe\n",
    "playerAge_df.head()                    #Returns AGE vs Player name DF      \n",
    "\n",
    "drop_df= df.drop_duplicates(['Player', 'Season'])    # Needed to drop players with duplicate salaries for the same season\n",
    "df=drop_df\n",
    "\n",
    "\n",
    "\n",
    "#====================================================================================================\n",
    "#====================Pivot Table used to view dataset================================================\n",
    "#=====================Not Needed, but I wanted to look at the dataset================================\n",
    "#====================================================================================================\n",
    "\n",
    "pivot_df=df.pivot(index=\"Player\", columns=\"Season\", values=\"Salary\")\n",
    "\n",
    "#NOT NEEDED, BUT WANTED\n",
    "#=====================================================================================================\n",
    "unique = df[\"Season\"].unique()                         # Just showing the number years in my raw data\n",
    "unique\n",
    "#=====================================================================================================                   \n",
    "Dwight= df.loc[df[\"Player\"] == \"Dwight Howard\"]         # I want to check the salary of a player over all the years to verify that my merged dataframes are putting data in the correct location\n",
    "Jarrett= df.loc[df[\"Player\"] == \"Jarrett Jack\"]         # I want to check the salary of a player over all the years to verify that my merged dataframes are putting data in the correct location\n",
    "\n",
    "\n",
    "#===========================================================================================================\n",
    "#============================The Next few cells created the dataframes for the yearly salaries==============\n",
    "#===========================================================================================================\n",
    "\n",
    "df2018= df.loc[df[\"Season\"] == \"2018\"]\n",
    "df2018= df2018.iloc[:,[0,1]]\n",
    "df2017= df.loc[df[\"Season\"] == \"2017\"]\n",
    "df2017= df2017.iloc[:,[0,1]]\n",
    "df2016= df.loc[df[\"Season\"] == \"2016\"]\n",
    "df2016= df2016.iloc[:,[0,1]]\n",
    "df2015= df.loc[df[\"Season\"] == \"2015\"]\n",
    "df2015= df2015.iloc[:,[0,1]]\n",
    "#===========================2014===========================================\n",
    "#===========================================================================\n",
    "df2014= df.loc[df[\"Season\"] == \"2014\"]\n",
    "df2014= df2014.iloc[:,[0,1]]\n",
    "#df2014.head()\n",
    "#===========================2013===========================================\n",
    "#===========================================================================\n",
    "df2013= df.loc[df[\"Season\"] == \"2013\"]\n",
    "df2013= df2013.iloc[:,[0,1]]\n",
    "#df2013.head()\n",
    "#===========================2012===========================================\n",
    "#===========================================================================\n",
    "df2012= df.loc[df[\"Season\"] == \"2012\"]\n",
    "df2012= df2012.iloc[:,[0,1]]\n",
    "#df2012.head()\n",
    "#===========================2011===========================================\n",
    "#===========================================================================\n",
    "df2011= df.loc[df[\"Season\"] == \"2011\"]\n",
    "df2011= df2011.iloc[:,[0,1]]\n",
    "#df2011.head()\n",
    "#===========================2010===========================================\n",
    "#===========================================================================\n",
    "df2010= df.loc[df[\"Season\"] == \"2010\"]\n",
    "df2010= df2010.iloc[:,[0,1]]\n",
    "#df2010.head()\n",
    "#===========================2009===========================================\n",
    "#===========================================================================\n",
    "df2009= df.loc[df[\"Season\"] == \"2009\"]\n",
    "df2009= df2009.iloc[:,[0,1]]\n",
    "#df2009.head()\n",
    "#===========================2008===========================================\n",
    "#===========================================================================\n",
    "df2008= df.loc[df[\"Season\"] == \"2008\"]\n",
    "df2008= df2008.iloc[:,[0,1]]\n",
    "#df2008.head()\n",
    "#===========================2007===========================================\n",
    "#===========================================================================\n",
    "df2007= df.loc[df[\"Season\"] == \"2007\"]\n",
    "df2007= df2007.iloc[:,[0,1]]\n",
    "#df2007.head()\n",
    "#===========================2006===========================================\n",
    "#===========================================================================\n",
    "df2006= df.loc[df[\"Season\"] == \"2006\"]\n",
    "df2006= df2006.iloc[:,[0,1]]\n",
    "#df2006.head()\n",
    "#===========================2005===========================================#\n",
    "#===========================================================================\n",
    "df2005= df.loc[df[\"Season\"] == \"2005\"]\n",
    "df2005= df2005.iloc[:,[0,1]]\n",
    "#df2005.head()\n",
    "#===========================2004===========================================\n",
    "#===========================================================================\n",
    "df2004= df.loc[df[\"Season\"] == \"2004\"]\n",
    "df2004= df2004.iloc[:,[0,1]]\n",
    "#df2004.head()\n",
    "#===========================2003===========================================\n",
    "#===========================================================================\n",
    "df2003= df.loc[df[\"Season\"] == \"2003\"]\n",
    "df2003= df2003.iloc[:,[0,1]]\n",
    "#df2003.head()\n",
    "#===========================2002===========================================\n",
    "#===========================================================================\n",
    "df2002= df.loc[df[\"Season\"] == \"2002\"]\n",
    "df2002= df2002.iloc[:,[0,1]]\n",
    "#df2002.head()\n",
    "#===========================2001===========================================\n",
    "#===========================================================================\n",
    "df2001= df.loc[df[\"Season\"] == \"2001\"]\n",
    "df2001= df2001.iloc[:,[0,1]]\n",
    "#df2001.head()\n",
    "#===========================2000===========================================\n",
    "#===========================================================================\n",
    "df2000= df.loc[df[\"Season\"] == \"2000\"]\n",
    "df2000= df2000.iloc[:,[0,1]]\n",
    "#df2000.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=============================MERGING OF THE DATASETS JUST CREATED FOR SALARY VS YEARS===============\n",
    "#====================================================================================================\n",
    "\n",
    "Compiled_df = df2018.merge(df2017, how='left', left_on=\"Player\", right_on='Player',suffixes=('_2018','_2017')).fillna(0)   #Left Merge Command. Left merges does not delete data\n",
    "Compiled_df = Compiled_df.merge(df2016, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2016')).fillna(0)   #Left Merge Command. Left merges does not delete data\n",
    "Compiled_df = Compiled_df.merge(df2015, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2015')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2014, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2014')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2013, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2013')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2012, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2012')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2011, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2011')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2010, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2010')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2009, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2009')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2008, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2008')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2007, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2007')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2006, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2006')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2005, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2005')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2004, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2004')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2003, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2003')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2002, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2002')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2001, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2001')).fillna(0)\n",
    "Compiled_df = Compiled_df.merge(df2000, how='left', left_on=\"Player\", right_on='Player',suffixes=('','_2000')).fillna(0)\n",
    "\n",
    "\n",
    "new_col=['Player','2018', '2017', '2016', '2015', '2014', '2013', '2012',\n",
    "        '2011', '2010', '2009', '2008', '2007', '2006', '2005', \n",
    "        '2004', '2003', '2002', '2001', '2000']\n",
    "                                 \n",
    "Compiled_df.columns = new_col\n",
    "#Compiled_df\n",
    "\n",
    "\n",
    "#====================================================================================================\n",
    "#==================Creating final Dataframe with a Merge Function====================================\n",
    "#=====Function conveniently deletes rows with index values that are not shared by both dataframes====\n",
    "#====================================================================================================\n",
    "\n",
    "\n",
    "Compiled_df = pd.merge(playerAge_df, Compiled_df, on=\"Player\")\n",
    "drop2_df= Compiled_df.drop_duplicates(['Player', 'Age']) # Needed to drop players with duplicate salaries for the same season\n",
    "Compiled_df=drop2_df                                      #Moving final dataframe into global function\n",
    "\n",
    "# Push the remade DataFrame to a new CSV file\n",
    "Compiled_df.to_csv(\"NBA_clean.csv\", encoding=\"utf-8\", index=False, header=True)\n",
    "\n",
    "print(\"NBA Data Cleaning ran successfully\")\n",
    "#END===============:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
