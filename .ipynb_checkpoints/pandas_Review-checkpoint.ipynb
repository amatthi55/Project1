{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Pandas files\n",
    "#DAY 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Modules\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Prompt user for video lookup\n",
    "video = input(\"What show or movie are you looking for? \")\n",
    "\n",
    "# Set path for file\n",
    "csvpath = os.path.join(\"..\", \"Resources\", \"netflix_ratings.csv\")\n",
    "print(csvpath)\n",
    "\n",
    "# Set variable to check if we found the video\n",
    "found = False\n",
    "\n",
    "# Open the CSV\n",
    "with open(csvpath, newline=\"\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "\n",
    "    # Loop through looking for the video\n",
    "    for row in csvreader:\n",
    "        if row[0] == video:\n",
    "            print(row[0]+ \" is rated \"+ row[1] + \" with a rating of \" + row[6])\n",
    "\n",
    "            # Set variable to confirm we have found the video\n",
    "            found = True\n",
    "\n",
    "    # If the video is never found, alert the user\n",
    "    if found == False:\n",
    "        print(\"We don't seem to have what you are looking for!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We can create a Pandas Series from a raw list\n",
    "data_series = pd.Series([\"UCLA\", \"UC Berkeley\", \"UC Irvine\", \"University of Central Florida\", \"Rutgers University\"])\n",
    "data_series\n",
    "\n",
    " # Convert a list of dictionarys into a dataframe\n",
    "states_dicts = [{\"STATE\":\"New Jersey\",\"ABBREVIATION\":\"NJ\"},{\"STATE\":\"New York\",\"ABBREVIATION\":\"NY\"}]\n",
    "\n",
    "df_states = pd.DataFrame(states_dicts)\n",
    "df_states\n",
    "\n",
    " # Convert a single dictionary containing lists into a dataframe\n",
    "df = pd.DataFrame(\n",
    "        {\"Dynasty\": [\"Early Dynastic Period\",\"Old Kingdom\"],\n",
    "         \"Pharoh\": [\"Thinis\",\"Memphis\"]\n",
    "        }\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame of frames using a dictionary of lists\n",
    "frame_df = pd.DataFrame({\n",
    "    \"Frame\":[\"Ornate\",\"Classical\",\"Modern\",\"Wood\",\"Cardboard\"],\n",
    "    \"Price\":[15.00, 12.50, 10.00, 5.00, 1.00],\n",
    "    \"Sales\":[100, 200, 150, 300, \"N/A\"]\n",
    "})\n",
    "frame_df\n",
    "\n",
    " # Create a DataFrame of paintings using a list of dictionaries\n",
    "painting_df = pd.DataFrame([\n",
    "    {\"Painting\":\"Mona Lisa (Knockoff)\", \"Price\":25, \"Popularity\":\"Very Popular\"},\n",
    "    {\"Painting\":\"Van Gogh (Knockoff)\", \"Price\":20, \"Popularity\":\"Popular\"},\n",
    "    {\"Painting\":\"Starving Artist\", \"Price\":10, \"Popularity\":\"Average\"},\n",
    "    {\"Painting\":\"Toddler Drawing\", \"Price\":1, \"Popularity\":\"Not Popular\"}\n",
    "])\n",
    "painting_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Save path to data set in a variable\n",
    "data_file = \"Resources/dataSet.csv\"\n",
    "\n",
    " # Use Pandas to read data\n",
    "data_file_pd = pd.read_csv(data_file)\n",
    "data_file_pd.head()\n",
    "\n",
    "# Display a statistical overview of the DataFrame\n",
    "data_file_pd.describe()\n",
    "\n",
    " # Reference a single column within a DataFrame\n",
    "data_file_pd[\"Amount\"].head()\n",
    "\n",
    " # Reference multiple columns within a DataFrame\n",
    "data_file_pd[[\"Amount\",\"Gender\"]].head()\n",
    "\n",
    "# The mean method averages the series \n",
    "average = data_file_pd[\"Amount\"].mean()\n",
    "average\n",
    "\n",
    " # The sum method adds every entry in the series\n",
    "total = data_file_pd[\"Amount\"].sum()\n",
    "total\n",
    "\n",
    " # The unique method shows every element of the series that appears only once\n",
    "unique = data_file_pd[\"Last Name\"].unique()\n",
    "unique\n",
    "\n",
    " # The value_counts method counts unique values in a column\n",
    "count = data_file_pd[\"Gender\"].value_counts()\n",
    "count\n",
    "\n",
    " # Calculations can also be performed on Series and added into DataFrames as new columns\n",
    "thousands_of_dollars = data_file_pd[\"Amount\"]/1000\n",
    "data_file_pd[\"Thousands of Dollars\"] = thousands_of_dollars\n",
    "\n",
    "data_file_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# A gigantic DataFrame of individuals' names, their trainers, their weight, and their days as gym members\n",
    "training_data = pd.DataFrame({\n",
    "    \"Name\":[\"Gino Walker\",\"Hiedi Wasser\",\"Kerrie Wetzel\",\"Elizabeth Sackett\",\"Jack Mitten\",\"Madalene Wayman\",\"Jamee Horvath\",\"Arlena Reddin\",\"Tula Levan\",\"Teisha Dreier\",\"Leslie Carrier\",\"Arlette Hartson\",\"Romana Merkle\",\"Heath Viviani\",\"Andres Zimmer\",\"Allyson Osman\",\"Yadira Caggiano\",\"Jeanmarie Friedrichs\",\"Leann Ussery\",\"Bee Mom\",\"Pandora Charland\",\"Karena Wooten\",\"Elizabet Albanese\",\"Augusta Borjas\",\"Erma Yadon\",\"Belia Lenser\",\"Karmen Sancho\",\"Edison Mannion\",\"Sonja Hornsby\",\"Morgan Frei\",\"Florencio Murphy\",\"Christoper Hertel\",\"Thalia Stepney\",\"Tarah Argento\",\"Nicol Canfield\",\"Pok Moretti\",\"Barbera Stallings\",\"Muoi Kelso\",\"Cicely Ritz\",\"Sid Demelo\",\"Eura Langan\",\"Vanita An\",\"Frieda Fuhr\",\"Ernest Fitzhenry\",\"Ashlyn Tash\",\"Melodi Mclendon\",\"Rochell Leblanc\",\"Jacqui Reasons\",\"Freeda Mccroy\",\"Vanna Runk\",\"Florinda Milot\",\"Cierra Lecompte\",\"Nancey Kysar\",\"Latasha Dalton\",\"Charlyn Rinaldi\",\"Erline Averett\",\"Mariko Hillary\",\"Rosalyn Trigg\",\"Sherwood Brauer\",\"Hortencia Olesen\",\"Delana Kohut\",\"Geoffrey Mcdade\",\"Iona Delancey\",\"Donnie Read\",\"Cesar Bhatia\",\"Evia Slate\",\"Kaye Hugo\",\"Denise Vento\",\"Lang Kittle\",\"Sherry Whittenberg\",\"Jodi Bracero\",\"Tamera Linneman\",\"Katheryn Koelling\",\"Tonia Shorty\",\"Misha Baxley\",\"Lisbeth Goering\",\"Merle Ladwig\",\"Tammie Omar\",\"Jesusa Avilla\",\"Alda Zabala\",\"Junita Dogan\",\"Jessia Anglin\",\"Peggie Scranton\",\"Dania Clodfelter\",\"Janis Mccarthy\",\"Edmund Galusha\",\"Tonisha Posey\",\"Arvilla Medley\",\"Briana Barbour\",\"Delfina Kiger\",\"Nia Lenig\",\"Ricarda Bulow\",\"Odell Carson\",\"Nydia Clonts\",\"Andree Resendez\",\"Daniela Puma\",\"Sherill Paavola\",\"Gilbert Bloomquist\",\"Shanon Mach\",\"Justin Bangert\",\"Arden Hokanson\",\"Evelyne Bridge\",\"Hee Simek\",\"Ward Deangelis\",\"Jodie Childs\",\"Janis Boehme\",\"Beaulah Glowacki\",\"Denver Stoneham\",\"Tarra Vinton\",\"Deborah Hummell\",\"Ulysses Neil\",\"Kathryn Marques\",\"Rosanna Dake\",\"Gavin Wheat\",\"Tameka Stoke\",\"Janella Clear\",\"Kaye Ciriaco\",\"Suk Bloxham\",\"Gracia Whaley\",\"Philomena Hemingway\",\"Claudette Vaillancourt\",\"Olevia Piche\",\"Trey Chiles\",\"Idalia Scardina\",\"Jenine Tremble\",\"Herbert Krider\",\"Alycia Schrock\",\"Miss Weibel\",\"Pearlene Neidert\",\"Kina Callender\",\"Charlotte Skelley\",\"Theodora Harrigan\",\"Sydney Shreffler\",\"Annamae Trinidad\",\"Tobi Mumme\",\"Rosia Elliot\",\"Debbra Putt\",\"Rena Delosantos\",\"Genna Grennan\",\"Nieves Huf\",\"Berry Lugo\",\"Ayana Verdugo\",\"Joaquin Mazzei\",\"Doris Harmon\",\"Patience Poss\",\"Magaret Zabel\",\"Marylynn Hinojos\",\"Earlene Marcantel\",\"Yuki Evensen\",\"Rema Gay\",\"Delana Haak\",\"Patricia Fetters\",\"Vinnie Elrod\",\"Octavia Bellew\",\"Burma Revard\",\"Lakenya Kato\",\"Vinita Buchner\",\"Sierra Margulies\",\"Shae Funderburg\",\"Jenae Groleau\",\"Louetta Howie\",\"Astrid Duffer\",\"Caron Altizer\",\"Kymberly Amavisca\",\"Mohammad Diedrich\",\"Thora Wrinkle\",\"Bethel Wiemann\",\"Patria Millet\",\"Eldridge Burbach\",\"Alyson Eddie\",\"Zula Hanna\",\"Devin Goodwin\",\"Felipa Kirkwood\",\"Kurtis Kempf\",\"Kasey Lenart\",\"Deena Blankenship\",\"Kandra Wargo\",\"Sherrie Cieslak\",\"Ron Atha\",\"Reggie Barreiro\",\"Daria Saulter\",\"Tandra Eastman\",\"Donnell Lucious\",\"Talisha Rosner\",\"Emiko Bergh\",\"Terresa Launius\",\"Margy Hoobler\",\"Marylou Stelling\",\"Lavonne Justice\",\"Kala Langstaff\",\"China Truett\",\"Louanne Dussault\",\"Thomasena Samaniego\",\"Charlesetta Tarbell\",\"Fatimah Lade\",\"Malisa Cantero\",\"Florencia Litten\",\"Francina Fraise\",\"Patsy London\",\"Deloris Mclaughlin\"],\n",
    "    \"Trainer\":['Bettyann Savory','Mariah Barberio','Gordon Perrine','Pa Dargan','Blanch Victoria','Aldo Byler','Aldo Byler','Williams Camire','Junie Ritenour','Gordon Perrine','Bettyann Savory','Mariah Barberio','Aldo Byler','Barton Stecklein','Bettyann Savory','Barton Stecklein','Gordon Perrine','Pa Dargan','Aldo Byler','Brittani Brin','Bettyann Savory','Phyliss Houk','Bettyann Savory','Junie Ritenour','Aldo Byler','Calvin North','Brittani Brin','Junie Ritenour','Blanch Victoria','Brittani Brin','Bettyann Savory','Blanch Victoria','Mariah Barberio','Bettyann Savory','Blanch Victoria','Brittani Brin','Junie Ritenour','Pa Dargan','Gordon Perrine','Phyliss Houk','Pa Dargan','Mariah Barberio','Phyliss Houk','Phyliss Houk','Calvin North','Williams Camire','Brittani Brin','Gordon Perrine','Bettyann Savory','Bettyann Savory','Pa Dargan','Phyliss Houk','Barton Stecklein','Blanch Victoria','Coleman Dunmire','Phyliss Houk','Blanch Victoria','Pa Dargan','Harland Coolidge','Calvin North','Bettyann Savory','Phyliss Houk','Bettyann Savory','Harland Coolidge','Gordon Perrine','Junie Ritenour','Harland Coolidge','Blanch Victoria','Mariah Barberio','Coleman Dunmire','Aldo Byler','Bettyann Savory','Gordon Perrine','Bettyann Savory','Barton Stecklein','Harland Coolidge','Aldo Byler','Aldo Byler','Pa Dargan','Junie Ritenour','Brittani Brin','Junie Ritenour','Gordon Perrine','Mariah Barberio','Mariah Barberio','Mariah Barberio','Bettyann Savory','Brittani Brin','Aldo Byler','Phyliss Houk','Blanch Victoria','Pa Dargan','Phyliss Houk','Brittani Brin','Barton Stecklein','Coleman Dunmire','Bettyann Savory','Bettyann Savory','Gordon Perrine','Blanch Victoria','Junie Ritenour','Phyliss Houk','Coleman Dunmire','Williams Camire','Harland Coolidge','Williams Camire','Aldo Byler','Harland Coolidge','Gordon Perrine','Brittani Brin','Coleman Dunmire','Calvin North','Phyliss Houk','Brittani Brin','Aldo Byler','Bettyann Savory','Brittani Brin','Gordon Perrine','Calvin North','Harland Coolidge','Coleman Dunmire','Harland Coolidge','Aldo Byler','Junie Ritenour','Blanch Victoria','Harland Coolidge','Blanch Victoria','Junie Ritenour','Harland Coolidge','Junie Ritenour','Gordon Perrine','Brittani Brin','Coleman Dunmire','Williams Camire','Junie Ritenour','Brittani Brin','Calvin North','Barton Stecklein','Barton Stecklein','Mariah Barberio','Coleman Dunmire','Bettyann Savory','Mariah Barberio','Pa Dargan','Barton Stecklein','Coleman Dunmire','Brittani Brin','Barton Stecklein','Pa Dargan','Barton Stecklein','Junie Ritenour','Bettyann Savory','Williams Camire','Pa Dargan','Calvin North','Williams Camire','Coleman Dunmire','Aldo Byler','Barton Stecklein','Coleman Dunmire','Blanch Victoria','Mariah Barberio','Mariah Barberio','Harland Coolidge','Barton Stecklein','Phyliss Houk','Pa Dargan','Bettyann Savory','Barton Stecklein','Harland Coolidge','Junie Ritenour','Pa Dargan','Mariah Barberio','Blanch Victoria','Williams Camire','Phyliss Houk','Phyliss Houk','Coleman Dunmire','Mariah Barberio','Gordon Perrine','Coleman Dunmire','Brittani Brin','Pa Dargan','Coleman Dunmire','Brittani Brin','Blanch Victoria','Coleman Dunmire','Gordon Perrine','Coleman Dunmire','Aldo Byler','Aldo Byler','Mariah Barberio','Williams Camire','Phyliss Houk','Aldo Byler','Williams Camire','Aldo Byler','Williams Camire','Coleman Dunmire','Phyliss Houk'],\n",
    "    \"Weight\":[128,180,193,177,237,166,224,208,177,241,114,161,162,151,220,142,193,193,124,130,132,141,190,239,213,131,172,127,184,157,215,122,181,240,218,205,239,217,234,158,180,131,194,171,177,110,117,114,217,123,248,189,198,127,182,121,224,111,151,170,188,150,137,231,222,186,139,175,178,246,150,154,129,216,144,198,228,183,173,129,157,199,186,232,172,157,246,239,214,161,132,208,187,224,164,177,175,224,219,235,112,241,243,179,208,196,131,207,182,233,191,162,173,197,190,182,231,196,196,143,250,174,138,135,164,204,235,192,114,179,215,127,185,213,250,213,153,217,176,190,119,167,118,208,113,206,200,236,159,218,168,159,156,183,121,203,215,209,179,219,174,220,129,188,217,250,166,157,112,236,182,144,189,243,238,147,165,115,160,134,245,174,238,157,150,184,174,134,134,248,199,165,117,119,162,112,170,224,247,217],\n",
    "    \"Membership(Days)\":[52,70,148,124,186,157,127,155,37,185,158,129,93,69,124,13,76,153,164,161,48,121,167,69,39,163,7,34,176,169,108,162,195,86,155,77,197,200,80,142,179,67,58,145,188,147,125,15,13,173,125,4,61,29,132,110,62,137,197,135,162,174,32,151,149,65,18,42,63,62,104,200,189,40,38,199,1,12,8,2,195,30,7,72,130,144,2,34,200,143,43,196,22,115,171,54,143,59,14,52,109,115,187,185,26,19,178,18,120,169,45,52,130,69,168,178,96,22,78,152,39,51,118,130,60,156,108,69,103,158,165,142,86,91,117,77,57,169,86,188,97,111,22,83,81,177,163,35,12,164,21,181,171,138,22,107,58,51,38,128,19,193,157,13,104,89,13,10,26,190,179,101,7,159,100,49,120,109,56,199,51,108,47,171,69,162,74,119,148,88,32,159,65,146,140,171,88,18,59,13]\n",
    "    })\n",
    "training_data.head(10)\n",
    "\n",
    " # Collecting a summary of all numeric data\n",
    "training_data.describe()\n",
    "\n",
    " # Finding how the names of the trainers\n",
    "training_data[\"Trainer\"].unique()\n",
    "\n",
    "# Finding how many students each trainer has\n",
    "training_data[\"Trainer\"].value_counts()\n",
    "\n",
    " # Finding the average weight of all students\n",
    "training_data[\"Weight\"].mean()\n",
    "\n",
    " # Converting the membership days into weeks and then adding a column to the DataFrame\n",
    "weeks = training_data[\"Membership (Days)\"]/7\n",
    "training_data[\"Membership (Weeks)\"] = weeks\n",
    "\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting a list of all columns within the DataFrame\n",
    "training_data.columns\n",
    "\n",
    "# Reorganizing the columns using double brackets\n",
    "organized_df = training_data[[\"Name\",\"Trainer\",\"Weight\",\"Membership(Days)\"]]\n",
    "organized_df.head()\n",
    "\n",
    " # Using .rename(columns={}) in order to rename columns\n",
    "renamed_df = organized_df.rename(columns={\"Membership(Days)\":\"Membership in Days\", \"Weight\":\"Weight in Pounds\"})\n",
    "renamed_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Store filepath in a variable\n",
    "file_one = \"Resources/books_clean.csv\"\n",
    "\n",
    "# Read our Data file with the pandas library\n",
    "# Not every CSV requires an encoding, but be aware this can come up\n",
    "file_one_df = pd.read_csv(file_one, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Export file as a CSV, without the Pandas index, but with the header\n",
    "file_one_df.to_csv(\"Output/books_again.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Dependencies\n",
    "import pandas as pd\n",
    "\n",
    " # Create a DataFrame with given columns and value\n",
    "hey_arnold = pd.DataFrame(\n",
    "    {\"Character_in_show\": [\"Arnold\", \"Gerald\", \"Helga\", \"Phoebe\", \"Harold\", \"Eugene\"],\n",
    "     \"color_of_hair\": [\"blonde\", \"black\",\"blonde\", \"black\", \"unknown\", \"red\"],\n",
    "     \"Height\": [\"average\", \"tallish\", \"tallish\", \"short\", \"tall\", \"short\"],\n",
    "     \"Football_Shaped_Head\": [True, False, False, False, False, False]\n",
    "    })\n",
    "\n",
    "hey_arnold\n",
    "\n",
    "# Rename columns for readability\n",
    "hey_arnold_renamed = hey_arnold.rename(columns={\"Character_in_show\": \"Character\",\n",
    "                                                \"color_of_hair\": \"Hair Color\",\n",
    "                                                \"Height\": \"Height\",\n",
    "                                                \"Football_Shaped_Head\": \"Football Head\"\n",
    "                                               })\n",
    "hey_arnold_renamed\n",
    "\n",
    "# Organize the columns so they are in a more logical order\n",
    "hey_arnold_alphabetical = hey_arnold_renamed[[\"Character\",\"Football Head\",\"Hair Color\",\"Height\"]]\n",
    "hey_arnold_alphabetical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Store filepath in a variable\n",
    "file_one = \"Resources/DataOne.csv\"\n",
    "\n",
    " # Read our Data file with the pandas library\n",
    "# Not every CSV requires an encoding, but be aware this can come up\n",
    "file_one_df = pd.read_csv(file_one, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Show just the header \n",
    "file_one_df.head()\n",
    "\n",
    " # Show a single column\n",
    "file_one_df[\"first_name\"].head(\n",
    "    \n",
    " # Show mulitple specific columns--note the extra brackets\n",
    "file_one_df[[\"first_name\", \"email\"]].head()\n",
    "    \n",
    " # Head does not change the DataFrame--it only displays it\n",
    "file_one_df.head()\n",
    "\n",
    "    # Export file as a CSV, without the Pandas index, but with the header\n",
    "file_one_df.to_csv(\"Output/fileOne.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Make a reference to the books.csv file path\n",
    "csv_path = \"../Resources/books.csv\"\n",
    "\n",
    "# Import the books.csv file as a DataFrame\n",
    "books_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "books_df.head()\n",
    "\n",
    "# Remove unecessary columns from the DataFrame and save the new DataFrame\n",
    "reduced_df = books_df[[\"isbn\",\"original_publication_year\",\"original_title\",\"authors\",\n",
    "                       \"ratings_1\",\"ratings_2\",\"ratings_3\",\"ratings_4\",\"ratings_5\"]]\n",
    "reduced_df.head()\n",
    "\n",
    "# Rename the headers to be more explanatory\n",
    "renamed_df = reduced_df.rename(columns={\"isbn\":\"ISBN\",\n",
    "                                       \"original_title\":\"Original Title\",\n",
    "                                       \"original_publication_year\":\"Publication Year\",\n",
    "                                       \"authors\":\"Authors\",\n",
    "                                       \"ratings_1\":\"One Star Reviews\",\n",
    "                                       \"ratings_2\":\"Two Star Reviews\",\n",
    "                                       \"ratings_3\":\"Three Star Reviews\",\n",
    "                                       \"ratings_4\":\"Four Star Reviews\",\n",
    "                                       \"ratings_5\":\"Five Star Reviews\",})\n",
    "renamed_df.head()\n",
    "\n",
    "# Push the remade DataFrame to a new CSV file\n",
    "renamed_df.to_csv(\"Output/books_clean.csv\", encoding=\"utf-8\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File to Load\n",
    "goodreads_path = \"../Resources/books_clean.csv\"\n",
    "\n",
    "# Read the modified GoodReads csv and store into Pandas DataFrame\n",
    "goodreads_df = pd.read_csv(goodreads_path, encoding=\"utf-8\")\n",
    "goodreads_df.head()\n",
    "\n",
    "# Calculate the number of unique authors in the DataFrame\n",
    "author_count = len(goodreads_df[\"Authors\"].unique())\n",
    "\n",
    "# Calculate the earliest/latest year a book was published\n",
    "earliest_year = goodreads_df[\"Publication Year\"].min()\n",
    "latest_year = goodreads_df[\"Publication Year\"].max()\n",
    "\n",
    "# Calculate the total reviews for the entire dataset\n",
    "total_reviews = goodreads_df[\"One Star Reviews\"].sum() + goodreads_df[\"Two Star Reviews\"].sum() + goodreads_df[\"Three Star Reviews\"].sum() + goodreads_df[\"Four Star Reviews\"].sum() +goodreads_df[\"Five Star Reviews\"].sum() \n",
    "    In [12]:\n",
    "   # Place all of the data found into a summary DataFrame\n",
    "\n",
    "# Place all of the data found into a summary DataFrame\n",
    "summary_table = pd.DataFrame({\"Total Unique Authors\":author_count,\n",
    "                             \"Earliest Year\":[earliest_year],\n",
    "                             \"Latest Year\":[latest_year],\n",
    "                             \"Total Reviews\":[total_reviews]})\n",
    "summary_table \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = \"Resources/sampleData.csv\"\n",
    " df_original = pd.read_csv(file)\n",
    "df_original.head()\n",
    "\n",
    "# Set new index to last_name\n",
    "df = df_original.set_index(\"last_name\")\n",
    "df.head()\n",
    "\n",
    "# Grab the data contained within the \"Berry\" row and the \"Phone Number\" column\n",
    "berry_phone = df.loc[\"Berry\", \"Phone Number\"]\n",
    "print(\"Using Loc: \" + berry_phone)\n",
    "\n",
    "also_berry_phone = df.iloc[1,2]\n",
    "print(\"Using Iloc: \" + also_berry_phone)\n",
    "\n",
    "# Grab the first five rows of data and the columns from \"id\" to \"Phone Number\"\n",
    "# The problem with using \"last_name\" as the index is that the values are not unique so duplicates are returned\n",
    "# If there are duplicates and loc[] is being used, Pandas will return an error\n",
    "richardson_to_morales = df.loc[[\"Richardson\", \"Berry\", \"Hudson\", \"Mcdonald\", \"Morales\"],[\"id\", \"first_name\", \"Phone Number\"]]\n",
    "print(richardson_to_morales)\n",
    "\n",
    "print()\n",
    "\n",
    "# Using iloc[] will not find duplicates since a numeric index is always unique\n",
    "also_richardson_to_morales = df.iloc[0:4,0:3]\n",
    "print(also_richardson_to_morales)\n",
    "\n",
    "# The following will select all rows for columns `first_name` and `Phone Number`\n",
    "df.loc[:, [\"first_name\", \"Phone Number\"]].head()\n",
    "\n",
    "# Loc and Iloc also allow for conditional statments to filter rows of data\n",
    "only_billys = df.loc[df[\"first_name\"] == \"Billy\",:]\n",
    "print(only_billys)\n",
    "\n",
    "print()\n",
    "\n",
    "# Multiple conditions can be set to narrow down or widen the filter\n",
    "only_billy_and_peter = df.loc[(df[\"first_name\"] == \"Billy\") | (df[\"first_name\"] == \"Peter\"), :]\n",
    "print(only_billy_and_peter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencie\n",
    "import pandas as pd\n",
    "\n",
    "# Load in file\n",
    "movie_file = \"../Resources/movie_scores.csv\"\n",
    "\n",
    " # Read and display the CSV with Pandas \n",
    "movie_file_pd = pd.read_csv(movie_file)\n",
    "movie_file_pd.head()\n",
    "\n",
    " # List all the columns in the table\n",
    "movie_file_pd.columns\n",
    "\n",
    " # We only want IMDb data, so create a new table that takes the Film and all the columns relating to IMDB\n",
    "imdb_table = movie_file_pd[[\"FILM\", \"IMDB\", \"IMDB_norm\", \"IMDB_norm_round\", \"IMDB_user_vote_count\"]]\n",
    "imdb_table.head()\n",
    "\n",
    "# We only like good movies, so find those that scored over 7, and ignore the norm rating\n",
    "good_movies = movie_file_pd.loc[movie_file_pd[\"IMDB\"] > 7, [\"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "good_movies.head()\n",
    "\n",
    " # Find less popular movies--i.e., those with fewer than 20K votes\n",
    "unknown_movies = good_movies.loc[good_movies[\"IMDB_user_vote_count\"] < 20000, [\"FILM\", \"IMDB\", \"IMDB_user_vote_count\"]]\n",
    "unknown_movies.head()\n",
    "\n",
    "# Finally, export this file to a spread so we can keep track of out new future watch list without the index\n",
    "unknown_movies.to_excel(\"../output/movieWatchlist.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    " # Name of the CSV file\n",
    "file = 'donors2008.csv'\n",
    "\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Preview of the DataFrame\n",
    "# Note that FIELD8 is likely a meaningless column\n",
    "df.head()\n",
    "\n",
    " # Delete extraneous column\n",
    "del df['FIELD8']\n",
    "df.head()\n",
    "\n",
    "# Identify incomplete rows\n",
    "df.count()\n",
    "\n",
    " # Drop all rows with missing information\n",
    "df = df.dropna(how='any')\n",
    "\n",
    " # Verify dropped rows\n",
    "df.count()\n",
    "\n",
    "# The Amount column is the wrong data type. It should be numeric.\n",
    "df.dtypes\n",
    "\n",
    "# Use pd.to_numeric() method to convert the datatype of the Amount column\n",
    "df['Amount'] = pd.to_numeric(df['Amount'])\n",
    "\n",
    "# Verify that the Amount column datatype has been made numeric\n",
    "df['Amount'].dtype\n",
    "\n",
    "# Display an overview of the Employers column\n",
    "df['Employer'].value_counts()\n",
    "\n",
    "# Clean up Employer category. Replace 'Self Employed' and 'Self' with 'Self-Employed'\n",
    "df['Employer'] = df['Employer'].replace({'Self Employed':'Self-Employed', 'Self': 'Self-Employed'})\n",
    "\n",
    " # Verify clean-up. \n",
    "df['Employer'].value_counts()\n",
    "\n",
    " df['Employer'] = df['Employer'].replace({'Not Employed': 'Unemployed'})\n",
    "df['Employer'].value_counts()\n",
    "\n",
    "# Display a statistical overview\n",
    "# We can infer the maximum allowable individual contribution from 'max'\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Reference the file where the CSV is located\n",
    "crime_csv_path = \"../Resources/crime_incident_data2017.csv\"\n",
    "\n",
    "# Import the data into a Pandas DataFrame\n",
    "crime_df = pd.read_csv(crime_csv_path)\n",
    "crime_df.head()\n",
    "\n",
    "#MISSING\n",
    "\n",
    "\n",
    " # Check to see if there are any values with mispelled or similar values in \"Offense Type\"\n",
    "no_null_crime_df[\"Offense Type\"].value_counts()\n",
    "\n",
    " # Combining similar offenses together\n",
    "no_null_crime_df[\"Offense Type\"] = no_null_crime_df[\"Offense Type\"].replace({\"Commercial Sex Acts\":\"Prostitution\",\"Assisting or Promoting Prostitution\":\"Prostitution\"})\n",
    "no_null_crime_df\n",
    "\n",
    "# Create a new DataFrame that looks into a specific neighborhood\n",
    "vernon_crime_df = no_null_crime_df.loc[no_null_crime_df[\"Neighborhood\"] == \"Vernon\"]\n",
    "vernon_crime_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Day2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Create a reference the CSV file desired\n",
    "csv_path = \"../Resources/ufoSightings.csv\"\n",
    "\n",
    "# Read the CSV into a Pandas DataFrame\n",
    "ufo_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print the first five rows of data to the screen\n",
    "ufo_df.head()\n",
    "\n",
    "# Remove the rows with missing data\n",
    "clean_ufo_df = ufo_df.dropna(how=\"any\")\n",
    "clean_ufo_df.count()\n",
    "\n",
    "# Filter the data so that only those sightings in the US are in a DataFrame\n",
    "usa_ufo_df = clean_ufo_df.loc[clean_ufo_df[\"country\"] == \"us\",:]\n",
    "usa_ufo_df\n",
    "\n",
    "# Count how many sightings have occured within each state\n",
    "state_counts = usa_ufo_df[\"state\"].value_counts()\n",
    "state_counts\n",
    "\n",
    "# Convert the state_counts Series into a DataFrame\n",
    "state_ufo_counts_df = pd.DataFrame(state_counts)\n",
    "state_ufo_counts_df.head()\n",
    "\n",
    "# Convert the column name into \"Sum of Sightings\"\n",
    "state_ufo_counts_df = state_ufo_counts_df.rename(columns={\"state\":\"Sum of Sightings\"})\n",
    "state_ufo_counts_df.head()\n",
    "\n",
    "# Want to add up the seconds UFOs are seen? There is a problem\n",
    "# Problem can be seen by examining datatypes within the DataFrame\n",
    "usa_ufo_df.dtypes\n",
    "\n",
    " # Using to_numeric() to convert a column's data into floats\n",
    "usa_ufo_df[\"duration (seconds)\"] = pd.to_numeric(usa_ufo_df[\"duration (seconds)\"])\n",
    "usa_ufo_df.dtypes\n",
    "\n",
    "# Now it is possible to find the sum of seconds\n",
    "usa_ufo_df[\"duration (seconds)\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a reference the CSV file desired\n",
    "csv_path = \"Resources/ufoSightings.csv\"\n",
    "\n",
    "# Read the CSV into a Pandas DataFrame\n",
    "ufo_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print the first five rows of data to the screen\n",
    "ufo_df.head()\n",
    "\n",
    " # Remove the rows with missing data\n",
    "clean_ufo_df = ufo_df.dropna(how=\"any\")\n",
    "clean_ufo_df.count()\n",
    "\n",
    "# Converting the \"duration (seconds)\" column's values to numeric\n",
    "clean_ufo_df[\"duration (seconds)\"] = pd.to_numeric(clean_ufo_df[\"duration (seconds)\"])\n",
    "\n",
    " # Filter the data so that only those sightings in the US are in a DataFrame\n",
    "usa_ufo_df = clean_ufo_df.loc[clean_ufo_df[\"country\"] == \"us\",:]\n",
    "\n",
    "usa_ufo_df.head()\n",
    "\n",
    " # Count how many sightings have occured within each state\n",
    "state_counts = usa_ufo_df[\"state\"].value_counts()\n",
    "state_counts.head()\n",
    "\n",
    "# Using GroupBy in order to separate the data into fields according to \"state\" values\n",
    "grouped_usa_df = usa_ufo_df.groupby(['state'])\n",
    "\n",
    "# The object returned is a \"GroupBy\" object and cannot be viewed normally...\n",
    "print(grouped_usa_df)\n",
    "\n",
    "# In order to be visualized, a data function must be used...\n",
    "grouped_usa_df.count().head(10)\n",
    "\n",
    "# Since \"duration (seconds)\" was converted to a numeric time, it can now be summed up per state\n",
    "state_duration = grouped_usa_df[\"duration (seconds)\"].sum()\n",
    "state_duration.head()\n",
    "\n",
    "# Creating a new DataFrame using both duration and count\n",
    "state_summary_table = pd.DataFrame({\"Number of Sightings\":state_counts,\n",
    "                                   \"Total Visit Time\":state_duration})\n",
    "state_summary_table.head()\n",
    "\n",
    " # It is also possible to group a DataFrame by multiple columns\n",
    "# This returns an object with multiple indexes, however, which can be harder to deal with\n",
    "grouped_international_data = clean_ufo_df.groupby(['country','state'])\n",
    "\n",
    "# Converting a GroupBy object into a DataFrame\n",
    "international_duration = pd.DataFrame(grouped_international_data[\"duration (seconds)\"].sum())\n",
    "international_duration.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Save file path to variable\n",
    "pokemon = \"../Resources/Pokemon.csv\"\n",
    "\n",
    "# Read with Pandas\n",
    "pokemon_pd = pd.read_csv(pokemon)\n",
    "pokemon_pd.head()\n",
    "\n",
    "# Create new table\n",
    "pokemon_type = pokemon_pd[[\"Type 1\", \"HP\", \"Attack\", \"Defense\", \"Sp. Atk\", \"Sp. Def\", \"Speed\"]]\n",
    "pokemon_type.head()\n",
    "\n",
    " # Create the GroupBy object based on the \"Type 1\" column\n",
    "pokemon_group = pokemon_type.groupby([\"Type 1\"])\n",
    "\n",
    "# Calculate averages for combat stats using the .mean() method\n",
    "pokemon_group.mean()\n",
    "\n",
    " # Converting the averages for each Pokemon Type into a DataFrame\n",
    "pokemon_comparison = pd.DataFrame(pokemon_group.mean())\n",
    "pokemon_comparison\n",
    "\n",
    "# Total number of points\n",
    "pokemon_comparison[\"Total\"] = pokemon_comparison[\"HP\"] + pokemon_comparison[\"Attack\"] + pokemon_comparison[\"Defense\"] + pokemon_comparison[\"Sp. Atk\"] + pokemon_comparison[\"Sp. Def\"] + pokemon_comparison[\"Speed\"]\n",
    "\n",
    "pokemon_comparison[\"Total\"]\n",
    "\n",
    "# Sort by strongest Pokemon, and reset index\n",
    "strongest_pokemon = pokemon_comparison.sort_values([\"Total\"], ascending=False)\n",
    "strongest_pokemon.reset_index(inplace=True)\n",
    "strongest_pokemon\n",
    "\n",
    "# Save output to Excel\n",
    "pokemon_comparison.to_excel(\"../output/pokemonRankings.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"Resources/Happiness_2017.csv\"\n",
    "happiness_df = pd.read_csv(csv_path)\n",
    "happiness_df.head()\n",
    "\n",
    "# Sorting the DataFrame based on \"Freedom\" column\n",
    "# Will sort from lowest to highest if no other parameter is passed\n",
    "freedom_df = happiness_df.sort_values(\"Freedom\")\n",
    "freedom_df.head()\n",
    "\n",
    "# To sort from highest to lowest, ascending=False must be passed in\n",
    "freedom_df = happiness_df.sort_values(\"Freedom\", ascending=False)\n",
    "freedom_df.head()\n",
    "\n",
    " # It is possible to sort based upon multiple columns\n",
    "family_and_generosity = happiness_df.sort_values([\"Family\",\"Generosity\"], ascending=False)\n",
    "family_and_generosity.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    " # Create reference to CSV file\n",
    "csv_path = \"../Resources/Soccer2018Data.csv\"\n",
    "\n",
    "# Import the CSV into a pandas DataFrame\n",
    "soccer_2018_df = pd.read_csv(csv_path, low_memory=False)\n",
    "soccer_2018_df\n",
    "\n",
    " # Sort the DataFrame by the values in the \"ST\" column to find the worst\n",
    "strikers_2018_df = strikers_2018_df.sort_values(\"ST\")\n",
    "\n",
    "# Reset the index so that the index is now based on the sorting locations\n",
    "strikers_2018_df = strikers_2018_df.reset_index(drop=True)\n",
    "\n",
    "strikers_2018_df.head()\n",
    "\n",
    " # Save all of the information collected on the worst striker\n",
    "worst_striker = strikers_2018_df.loc[0,:]\n",
    "worst_striker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "bitcoin_csv = \"../Resources/bitcoin_cash_price.csv\"\n",
    "dash_csv = \"../Resources/dash_price.csv\"\n",
    "\n",
    "bitcoin_df = pd.read_csv(bitcoin_csv)\n",
    "dash_df = pd.read_csv(dash_csv)\n",
    "\n",
    "bitcoin_df.head()\n",
    "\n",
    "dash_df.head()\n",
    "\n",
    " # Merge the two DataFrames together based on the Dates they share\n",
    "crypto_df = pd.merge(bitcoin_df, dash_df, on=\"Date\")\n",
    "crypto_df.head()\n",
    "\n",
    "# Rename columns so that they are differentiated\n",
    "crypto_df = crypto_df.rename(columns={\"Open_x\":\"Bitcoin Open\",\"High_x\":\"Bitcoin High\",\"Low_x\":\"Bitcoin Low\",\n",
    "                                      \"Close_x\":\"Bitcoin Close\",\"Volume_x\":\"Bitcoin Volume\",\"Market Cap_x\":\"Bitcoin Market Cap\"})\n",
    "\n",
    "crypto_df = crypto_df.rename(columns={\"Open_y\":\"Dash Open\",\"High_y\":\"Dash High\",\"Low_y\":\"Dash Low\",\n",
    "                                      \"Close_y\":\"Dash Close\",\"Volume_y\":\"Dash Volume\",\"Market Cap_y\":\"Dash Market Cap\"})\n",
    "\n",
    "crypto_df.head()\n",
    "\n",
    "# Collecting best open for Bitcoin and Dash\n",
    "bitcoin_open = crypto_df[\"Bitcoin Open\"].max()\n",
    "dash_open = crypto_df[\"Dash Open\"].max()\n",
    "\n",
    "# Collecting best close for Bitcoin and Dash\n",
    "bitcoin_close = crypto_df[\"Bitcoin Close\"].max()\n",
    "dash_close = crypto_df[\"Dash Close\"].max()\n",
    "\n",
    "# Collecting the total volume for Bitcoin and Dash\n",
    "bitcoin_volume = round(crypto_df[\"Bitcoin Volume\"].sum()/1000000, 2)\n",
    "dash_volume = round(crypto_df[\"Dash Volume\"].sum()/1000000, 2)\n",
    "\n",
    "# Creating a summary DataFrame using above values\n",
    "summary_df = pd.DataFrame({\"Best Bitcoin Open\":[bitcoin_open],\n",
    "                          \"Best Bitcoin Close\":[bitcoin_close],\n",
    "                          \"Total Bitcoin Volume\":str(bitcoin_volume)+\" million\",\n",
    "                          \"Best Dash Open\":[dash_open],\n",
    "                          \"Best Dash Close\":[dash_close],\n",
    "                          \"Total Dash Volume\":str(dash_volume)+\" million\"})\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'Regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], \n",
    "        'Company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], \n",
    "        'Name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], \n",
    "        'Test Score': [25, 94, 57, 62, 70, 30, 94, 57, 62, 70, 62, 70]}\n",
    "regiment_df = pd.DataFrame(raw_data)\n",
    "regiment_df\n",
    "\n",
    "# Create the bins in which Data will be held\n",
    "# Bins are 0 to 25, 25 to 50, 50 to 75, 75 to 100\n",
    "bins = [0, 25, 50, 75, 100]\n",
    "\n",
    "# Create the names for the four bins\n",
    "group_names = ['Low', 'Okay', 'Good', 'Great']\n",
    "\n",
    "# Cut postTestScore and place the scores into bins\n",
    "pd.cut(regiment_df[\"Test Score\"], bins, labels=group_names)\n",
    "\n",
    "regiment_df[\"Test Score Summary\"] = pd.cut(regiment_df[\"Test Score\"], bins, labels=group_names)\n",
    "regiment_df\n",
    "\n",
    " # Creating a group based off of the bins\n",
    "regiment_groups = regiment_df.groupby(\"Test Score Summary\")\n",
    "regiment_groups.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a path to the csv and read it into a Pandas DataFrame\n",
    "csv_path = \"../Resources/ted_talks.csv\"\n",
    "ted_df = pd.read_csv(csv_path)\n",
    "\n",
    "ted_df.head()\n",
    "\n",
    "# Figure out the minimum and maximum views for a TED Talk\n",
    "print(ted_df[\"views\"].max())\n",
    "print(ted_df[\"views\"].min())\n",
    "\n",
    "# Create bins in which to place values based upon TED Talk views\n",
    "bins = [0,200000,400000,600000,800000,1000000,2000000,3000000,4000000,5000000,50000000]\n",
    "\n",
    "# Create labels for these bins\n",
    "group_labels = [\"0 to 200k\",\"200k to 400k\",\"400k to 600k\",\"600k to 800k\",\"800k to 1mil\",\"1mil to 2mil\",\n",
    "                \"2mil to 3mil\",\"3mil to 4mil\",\"4mil to 5mil\",\"5mil to 50mil\"]\n",
    "\n",
    " # Slice the data and place it into bins\n",
    "pd.cut(ted_df[\"views\"],bins,labels=group_labels).head()\n",
    "\n",
    "# Place the data series into a new column inside of the DataFrame\n",
    "ted_df[\"View Group\"] = pd.cut(ted_df[\"views\"],bins,labels=group_labels)\n",
    "ted_df.head()\n",
    "\n",
    " # Create a GroupBy object based upon \"View Group\"\n",
    "ted_group = ted_df.groupby(\"View Group\")\n",
    "\n",
    "# Find how many rows fall into each bin\n",
    "print(ted_group[\"comments\"].count())\n",
    "\n",
    "# Get the average of each column within the GroupBy object\n",
    "ted_group.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import pandas as pd\n",
    "\n",
    " # Mapping lets you format an entire DataFrame\n",
    "file = \"sample_data.csv\"\n",
    "file_df = pd.read_csv(file)\n",
    "file_df.head()\n",
    "\n",
    "# Use Map to format all the columns\n",
    "file_df[\"avg_cost\"] = file_df[\"avg_cost\"].map(\"${:.2f}\".format)\n",
    "file_df[\"population\"] = file_df[\"population\"].map(\"{:,}\".format)\n",
    "file_df[\"other\"] = file_df[\"other\"].map(\"{:.2f}\".format)\n",
    "file_df.head() \n",
    "\n",
    " # Mapping has changed the datatypes of the columns to strings\n",
    "file_df.dtypes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "# The path to our CSV file\n",
    "file = \"Resources/KickstarterData.csv\"\n",
    "\n",
    "# Read our Kickstarter data into pandas\n",
    "df = pd.read_csv(file)\n",
    "df.head()\n",
    "\n",
    " # Get a list of all of our columns for easy reference\n",
    "df.columns\n",
    "\n",
    " # Extract \"name\", \"goal\", \"pledged\", \"state\", \"country\", \"staff_pick\",\n",
    "# \"backers_count\", and \"spotlight\"\n",
    "reduced_kickstarter_df = df.loc[:,[\"name\", \"goal\", \"pledged\", \"state\", \"country\", \"staff_pick\",\"backers_count\", \"spotlight\"]]\n",
    "reduced_kickstarter_df\n",
    "\n",
    ":\n",
    "   # Collect only those projects that were hosted in the US\n",
    "hosted_in_us = reduced_kickstarter_df.loc[reduced_kickstarter_df[\"country\"] == \"US\"]\n",
    "hosted_in_us.head()\n",
    "\n",
    " # Create a new column that finds the average amount pledged to a project\n",
    "hosted_in_us[\"average_donation\"] = hosted_in_us['pledged']/hosted_in_us['backers_count']\n",
    "hosted_in_us.head()\n",
    "\n",
    " # Format our \"average_donation\", \"goal\", and \"pledged\" columns\n",
    "# to go to two decimal places, include a dollar sign, and use comma notation\n",
    "hosted_in_us[\"average_donation\"] = hosted_in_us[\"average_donation\"].map(\"$ {:,.2f}\".format)\n",
    "hosted_in_us[\"goal\"] = hosted_in_us[\"goal\"].map(\"$ {:,.2f}\".format)\n",
    "hosted_in_us[\"pledged\"] = hosted_in_us[\"pledged\"].map(\"$ {:,.2f}\".format)\n",
    "hosted_in_us.head()\n",
    "\n",
    "# Calculate the total number of backers for all US projects\n",
    "hosted_in_us[\"backers_count\"].sum()\n",
    "\n",
    " # Calculate the average number of backers for all US projects\n",
    "hosted_in_us[\"backers_count\"].mean()\n",
    "\n",
    " # Collect only those US campaigns that have been picked as a \"Staff Pick\"\n",
    "picked_by_staff = hosted_in_us.loc[hosted_in_us[\"staff_pick\"] == True]\n",
    "picked_by_staff\n",
    "\n",
    " # Group by the state of the campaigns and see if staff picks matter (Seems to matter quite a bit)\n",
    "state_groups = picked_by_staff.groupby(\"state\")\n",
    "state_groups[\"name\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Reference to CSV and reading CSV into Pandas DataFrame\n",
    "csv_path = \"Resources/flavors_of_cacao.csv\"\n",
    "chocolate_ratings_df = pd.read_csv(csv_path)\n",
    "chocolate_ratings_df.head(10)\n",
    "\n",
    "chocolate_ratings_df.columns\n",
    "\n",
    "# Converting the \"Cocoa Percent\" column to floats\n",
    "chocolate_ratings_df[\"Cocoa Percent\"] = chocolate_ratings_df[\"Cocoa Percent\"].replace('%','',regex=True).astype('float')\n",
    "\n",
    "# Finding the average cocoa percent\n",
    "chocolate_ratings_df[\"Cocoa Percent\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Create a reference to the CSV and import it into a Pandas DataFrame\n",
    "csv_path = \"../Resources/EclipseBugs.csv\"\n",
    "eclipse_df = pd.read_csv(csv_path)\n",
    "eclipse_df.head()\n",
    "\n",
    "# Get a reference to the column names\n",
    "eclipse_df.columns\n",
    "\n",
    "# Removing the newlines from column headers\n",
    "eclipse_df = eclipse_df.rename(columns={\"Bug\\nID\":\"Bug ID\",\n",
    "                                       \"Assignee\\nReal\\nName\":\"Assignee Real Name\",\n",
    "                                       \"Number of\\nComments\":\"Number of Comments\",\n",
    "                                       \"Reporter\\nReal\\nName\":\"Reporter Real Name\",\n",
    "                                       \"Target\\nMilestone\":\"Target Milestone\"})\n",
    "eclipse_df.columns\n",
    "\n",
    "# Finding the average number of comments per bug\n",
    "average_comments = eclipse_df[\"Number of Comments\"].mean()\n",
    "average_comments\n",
    "\n",
    "# Grouping the DataFrame by \"Assignee\"\n",
    "assignee_group = eclipse_df.groupby(\"Assignee\")\n",
    "\n",
    "# Count how many of each component Assignees worked on and create DataFrame of the data\n",
    "assignee_work = pd.DataFrame(assignee_group[\"Component\"].value_counts())\n",
    "assignee_work.head()\n",
    "\n",
    " # Rename the \"Component\" column to \"Component Bug Count\"\n",
    "assignee_work = assignee_work.rename(columns={\"Component\":\"Component Bug Count\"})\n",
    "assignee_work.head()\n",
    "\n",
    " # Find the percentage of bugs overall fixed by each Assignee\n",
    "total_bugs = eclipse_df[\"Assignee\"].count()\n",
    "bugs_per_user = assignee_group[\"Assignee\"].count()\n",
    "\n",
    "user_bug_percent = pd.DataFrame((bugs_per_user/total_bugs)*100)\n",
    "user_bug_percent.head()\n",
    "\n",
    "# Rename the \"Assignee\" column to \"Percent of Total Bugs Assigned\"\n",
    "user_bug_percent = user_bug_percent.rename(columns={\"Assignee\":\"Percent of Total Bugs Assigned\"})\n",
    "\n",
    "# Reset the index for this DataFrame so \"Assignee\" is a column\n",
    "user_bug_percent = user_bug_percent.reset_index()\n",
    "user_bug_percent.head()\n",
    "\n",
    "# Reset the index of \"assignee_group\" so that \"Assignee\" and \"Component\" are columns\n",
    "assignee_work = assignee_work.reset_index()\n",
    "assignee_work.head()\n",
    "\n",
    "# Merge the \"Percent of Total Bugs Assigned\" into the DataFrame\n",
    "assignee_work = assignee_work.merge(user_bug_percent,on=\"Assignee\")\n",
    "\n",
    "# Remove the extra columns\n",
    "assignee_work = assignee_work[[\"Assignee\",\"Percent of Total Bugs Assigned\",\n",
    "                               \"Component\",\"Component Bug Count\"]]\n",
    "assignee_work.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "# load CSV\n",
    "new_coders = \"../Resources/2016-FCC-New-Coders-Survey-Data.csv\"\n",
    "\n",
    "\n",
    "# Read with pandas\n",
    "new_coders_pd = pd.read_csv(new_coders, encoding=\"iso-8859-1\", low_memory=False)\n",
    "new_coders_pd.head()\n",
    "\n",
    "# Inspect all columns\n",
    "new_coders_pd.columns\n",
    "\n",
    "# Extract only columns 0, 1, 2, 3, 4, 7, 8, 9, 10,11, 29, 30, 32, 36, 37, 45, 48, 56, 110, 111 \n",
    "reduced_coders_pd = new_coders_pd.iloc[:,[0,1,2,3,4,6,7,8,9,10,11,29,30,32,36, 37, 45, 48, 56, 110, 111]]\n",
    "reduced_coders_pd.head()\n",
    "\n",
    " # Change \"0\" to \"No\" and \"1\" to \"Yes\" in response columns\n",
    "reduced_coders_pd= reduced_coders_pd.replace({0: \"No\", 1: \"Yes\"})\n",
    "reduced_coders_pd.head()\n",
    "\n",
    "# Calculate total number of respondents in survey\n",
    "total_surveyed = len(reduced_coders_pd)\n",
    "total_surveyed\n",
    "\n",
    "# Extract rows corresponding only to people who attended a bootcamp\n",
    "attended_bootcamp = reduced_coders_pd.loc[reduced_coders_pd[\"AttendedBootcamp\"] == \"Yes\"]\n",
    "attended_bootcamp.head()\n",
    "\n",
    "# Calculate average age of attendees \n",
    "average_age = attended_bootcamp[\"Age\"].mean()\n",
    "print(average_age)\n",
    "\n",
    "# Calculate how many people attended a bootcamp\n",
    "amount_attended = attended_bootcamp[\"AttendedBootcamp\"].count()\n",
    "print(amount_attended)\n",
    "\n",
    " # Calculate how many attendees hold degrees\n",
    "holds_degree = attended_bootcamp[\"SchoolDegree\"].count()\n",
    "print(holds_degree)\n",
    "\n",
    "# Count number of attendees who self-identify as male; female; or are of non-binary gender identification  \n",
    "total_gender = attended_bootcamp[\"Gender\"].count()\n",
    "male = attended_bootcamp[\"Gender\"].value_counts()['male']\n",
    "female = attended_bootcamp[\"Gender\"].value_counts()['female']\n",
    "non_gender_specific = total_gender - male - female\n",
    "print(f\" Total: {total_gender}\\n Male: {male}\\n Female: {female}\\n non_specfic: {non_gender_specific}\")\n",
    "\n",
    " # Calculate percentage of respondents who attended a bootcamp \n",
    "percent_attended = amount_attended / total_surveyed * 100\n",
    "print(percent_attended)\n",
    "\n",
    "# Calculate percentage of respondents belonging to each gender\n",
    "male_percent = (male/total_gender) * 100\n",
    "female_percent = (female/total_gender) * 100\n",
    "non_gender_specific_percent = (non_gender_specific/total_gender) * 100\n",
    "print(f\" % Male: {male_percent}\\n % Female: {female_percent}\\n % non_specifc: {non_gender_specific}\")\n",
    "\n",
    "# Calculate percentage of attendees with a college degree\n",
    "degree_percentage = (holds_degree/amount_attended) * 100\n",
    "print(degree_percentage)\n",
    "\n",
    " # Calculate average post-bootcamp salary\n",
    "avg_salary = attended_bootcamp[\"BootcampPostSalary\"].mean()\n",
    "print(avg_salary)\n",
    "\n",
    "\n",
    "# Create a new table consolodating above calculations\n",
    "bootcamp_breakdown = pd.DataFrame({\"Total Surveyed\": [total_surveyed],\n",
    "                                   \"Total Bootcamp attendees\": [amount_attended],\n",
    "                                   \"% attended bootcamp\": [percent_attended],\n",
    "                                   \"Avg. Age\": [average_age],\n",
    "                                   \"% Male\": [male_percent],\n",
    "                                   \"% Female\":[female_percent],\n",
    "                                   \"% Non Gender Specific\":[non_gender_specific_percent],\n",
    "                                   \"Has a degree\": [degree_percentage],\n",
    "                                   \"Average Post Bootcamp Salary\": [avg_salary]\n",
    "})\n",
    "bootcamp_breakdown = bootcamp_breakdown[[\"Total Surveyed\", \n",
    "                                         \"Total Bootcamp attendees\", \n",
    "                                         \"% attended bootcamp\", \n",
    "                                         \"Avg. Age\", \n",
    "                                         \"Has a degree\", \n",
    "                                         \"% Male\", \n",
    "                                         \"% Female\",\n",
    "                                         \"% Non Gender Specific\",\n",
    "                                         \"Average Post Bootcamp Salary\"]]\n",
    "bootcamp_breakdown = bootcamp_breakdown.round(2)\n",
    "\n",
    "bootcamp_breakdown\n",
    "\n",
    "\n",
    "# Improve formatting before outputting spreadsheet\n",
    "bootcamp_breakdown[\"% Male\"] = bootcamp_breakdown[\"% Male\"].map(\"{0:,.2f}%\".format)\n",
    "bootcamp_breakdown[\"% Female\"] = bootcamp_breakdown[\"% Female\"].map(\"{0:,.2f}%\".format)\n",
    "bootcamp_breakdown[\"% attended bootcamp\"] = bootcamp_breakdown[\"% attended bootcamp\"].map(\"{0:,.2f}%\".format)\n",
    "bootcamp_breakdown[\"% Non Gender Specific\"] = bootcamp_breakdown[\"% Non Gender Specific\"].map(\"{0:,.2f}%\".format)\n",
    "bootcamp_breakdown[\"Has a degree\"] = bootcamp_breakdown[\"Has a degree\"].map(\"{0:,.2f}%\".format)\n",
    "bootcamp_breakdown[\"Average Post Bootcamp Salary\"] = bootcamp_breakdown[\"Average Post Bootcamp Salary\"].map(\"${0:,.0f}\".format)\n",
    "bootcamp_breakdown\n",
    "\n",
    "\n",
    "# Export to Excel\n",
    "bootcamp_breakdown.to_excel(\"output/BootcampOutputPart1.xlsx\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame of the different boot camps that had a significant number of attendees\n",
    "bootcamp_name = pd.DataFrame(reduced_coders_pd[\"BootcampName\"].value_counts())\n",
    "\n",
    "bootcamp_name.reset_index(inplace=True)\n",
    "bootcamp_name.columns = [\"BootcampName\", \"Count\"]\n",
    "msg = 'Free Code Camp is not a bootcamp - please scroll up and change answer to \"no\"'\n",
    "\n",
    "# Extract only schools with a sufficient number of responses\n",
    "bootcamp_name = bootcamp_name.loc[(bootcamp_name[\"Count\"] > 9) & \n",
    "                  (bootcamp_name[\"BootcampName\"] != msg)]\n",
    "bootcamp_name.head()\n",
    "\n",
    "# Count how many attendees of each bootcamp would recommend the bootcamp\n",
    "recommend_bootcamp = attended_bootcamp.replace({\"Yes\": 1, \"No\": 0})\n",
    "recommend_bootcamp = pd.DataFrame(recommend_bootcamp.groupby(\"BootcampName\")[\"BootcampRecommend\"].sum())\n",
    "\n",
    "recommend_bootcamp.reset_index(inplace=True)\n",
    "recommend_bootcamp.columns=[\"BootcampName\", \"Recommenders\"]\n",
    "\n",
    "recommend_bootcamp.head()\n",
    "\n",
    " # Merge the two created data frames on the name of tbe bootcamp\n",
    "merged_df = pd.merge(bootcamp_name, recommend_bootcamp, on=\"BootcampName\")\n",
    "merged_df.head()\n",
    "\n",
    "# Calculate percentage of eac bootcamp's students who are recommenders\n",
    "merged_df[\"% Recommend\"] = merged_df[\"Recommenders\"] / merged_df[\"Count\"] * 100\n",
    "\n",
    "# Sort results in descending order\n",
    "merged_df = merged_df.sort_values([\"% Recommend\"], ascending=False).round(2)\n",
    "\n",
    "# Format for percentages\n",
    "merged_df[\"% Recommend\"] = merged_df[\"% Recommend\"].map(\"{0:,.2f}%\".format)\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "# Export to excel and remove index\n",
    "merged_df.to_excel(\"output/BootcampOutputPart2.xlsx\", index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
